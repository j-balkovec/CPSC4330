{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 4330 | HW3 | Spark\n",
    "\n",
    "The file `College_2015_16.csv` (posted on Canvas) contains the following fields:\n",
    "\n",
    "- `Unique ID`\n",
    "- `Name`\n",
    "- `City`\n",
    "- `State`\n",
    "- `Zip`\n",
    "- `Admission rate`\n",
    "- `Average SAT score`\n",
    "- `Enrollment`\n",
    "- `CostA`\n",
    "- `CostP`\n",
    "\n",
    "**The last two columns are the cost of public and private universities. If one is non-null, the other should be\n",
    "null. If both are null, that's a missing value. If both are non-null, use either value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark --v: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "# Imports & Logger Config\n",
    "# pyspark                   3.5.4              pyhd8ed1ab_0 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, avg, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "import pyspark\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "print(\"pyspark --v: \" + pyspark.__version__)\n",
    "\n",
    "COLLEGE_2015 = r'College_2015_16.csv'\n",
    "COLLEGE_2017 = r'College_2017_18.csv'\n",
    "\n",
    "# --------------------- CONFIG --------------------- #\n",
    "\n",
    "# make sure the 'logs' directory exists\n",
    "log_directory = \"logs\"\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "def setup_logger(name, log_filename):\n",
    "    log_file = os.path.join(log_directory, log_filename)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # file handler\n",
    "    file_handler = logging.FileHandler(log_file, mode='w')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "    # prevent duplicate handlers\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# separate loggers for each agent\n",
    "spark_logger = setup_logger('current-spark-session (hw3_2.ipynb)', 'spark-hw3-2.log')\n",
    "# --------------------- CONFIG --------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "\n",
    "# Line: 100654,Alabama A & M University,Normal,AL,35762,0.9027,929,4824,22886,NULL\n",
    "\n",
    "# - Unique ID\n",
    "# - Name\n",
    "# - City\n",
    "# - State\n",
    "# - Zip\n",
    "# - Admission rate\n",
    "# - Average SAT score\n",
    "# - Enrollment\n",
    "# - CostA\n",
    "# - CostP\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"city\", StringType(), True),\n",
    "  StructField(\"state\", StringType(), True),\n",
    "  StructField(\"zip\", StringType(), True),\n",
    "  StructField(\"admissionRate\", DoubleType(), True),\n",
    "  StructField(\"averageSAT\", IntegerType(), True),\n",
    "  StructField(\"enrollment\", IntegerType(), True),\n",
    "  StructField(\"costA\", IntegerType(), True),\n",
    "  StructField(\"costP\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# initialize spark session\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mcollege-data-analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                     \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# to see errors in the console\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbalkovec/Desktop/CPSC4330/HW3/src/hw3_2.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39msetLogLevel(\u001b[39m\"\u001b[39m\u001b[39mINFO\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    516\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 104\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mJAVA_GATEWAY_EXITED\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize spark session\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"college-data-analysis\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# to see errors in the console\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# load data\n",
    "file_path = \"../\" + COLLEGE_2015\n",
    "df = spark.read.csv(file_path, header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Convert the lines in the file to a tuple of fields, and only keep these attributes: ID, name, state,\n",
    "enrollment, and cost, where cost is either costA or costP as above. If enrollment cannot be\n",
    "converted to an int, set it to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data\n",
    "df_filtered = df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"state\"),\n",
    "    when(col(\"enrollment\").isNotNull(), col(\"enrollment\")).otherwise(None).alias(\"enrollment\"),\n",
    "    when(col(\"costA\").isNotNull(), col(\"costA\")).otherwise(col(\"costP\")).alias(\"cost\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Find how many records were filtered due to the invalid number of fields in the data (the file has 10 fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = df.count()\n",
    "valid_records = df_filtered.count()\n",
    "filtered_records = total_records - valid_records\n",
    "\n",
    "print(\"-- records filtered due to invalid fields:\", json.dumps(filtered_records, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Find how many records are there from the state of California?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_count = df_filtered.filter(col(\"state\") == \"CA\").count()\n",
    "print(\"-- number of records from CA:\", json.dumps(ca_count, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What percentage of the records have a non-null enrollment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid_enrollment = df_filtered.filter(col(\"enrollment\").isNotNull()).count()\n",
    "percentage_enrollment = (total_valid_enrollment / valid_records) * 100\n",
    "print(\"-- percentage of records with non-null enrollment:\", json.dumps(percentage_enrollment, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "What is the name and cost of the 5 most expensive universities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.orderBy(desc(\"cost\")).select(\"name\", \"cost\").show(5) #check for null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Find the number of universities in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupBy(\"state\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Find the total number of enrollments in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupBy(\"state\").sum(\"enrollment\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Find the average enrollment for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupBy(\"state\").agg(avg(\"enrollment\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Another file “College_2017_18.csv” (posted on Canvas) has the college enrollment data of year\n",
    "2017 - 2018. Write code to calculate percent of change in enrollment from year 2015 – 2016 to\n",
    "the year 2017 – 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = spark.read.csv(\"College_2017_18.csv\", schema=schema, header=False)\n",
    "df_2017_filtered = df_2017.select(\n",
    "    col(\"id\"),\n",
    "    col(\"enrollment\").alias(\"enrollment-2017\")\n",
    ")\n",
    "\n",
    "# Join datasets on ID\n",
    "df_joined = df_filtered.join(df_2017_filtered, \"id\", \"inner\")\n",
    "\n",
    "# Compute percent change\n",
    "df_change = df_joined.withColumn(\"percent-change\", ((col(\"enrollment-2017\") - col(\"enrollment\")) / col(\"enrollment\")) * 100)\n",
    "df_change.select(\"id\", \"enrollment\", \"enrollment-2017\", \"percent-change\").show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
